#!/usr/bin/env node

/**
 * Perfect Prompt Generator CLI
 * 
 * One-command UX: promptgen run frames/tf_qa_policy.json > out.json
 */

import { Command } from 'commander';
import { PerfectPromptGenerator, TaskFrameBuilder, Utils } from './lib/index';
import * as fs from 'fs';
import * as path from 'path';

const program = new Command();

// Mock LLM Provider for demonstration
class MockLLMProvider {
  async generate(request: any) {
    // In production, this would call OpenAI, Anthropic, etc.
    return {
      content: JSON.stringify({
        answer: "This is a mock response for demonstration purposes. In production, this would be generated by the actual LLM.",
        citations: [
          { chunk_id: "policy_2024_07#p12", relevance: 0.95, excerpt: "Sample policy excerpt" }
        ],
        confidence: 0.85,
        reasoning: "Mock reasoning process",
        flags: [],
        related_questions: ["What about...?", "How does...?"]
      }),
      usage: { prompt_tokens: 500, completion_tokens: 150, total_tokens: 650 },
      model: request.model,
      latency_ms: 1200,
      cost_usd: 0.0065
    };
  }
}

program
  .name('promptgen')
  .description('Perfect Prompt Generator CLI')
  .version('1.0.0');

program
  .command('run')
  .description('Execute a TaskFrame and generate response')
  .argument('<taskframe>', 'Path to TaskFrame JSON file')
  .option('-o, --output <path>', 'Output file path (default: stdout)')
  .option('--telemetry', 'Enable telemetry recording', true)
  .action(async (taskframePath, options) => {
    try {
      // Load TaskFrame
      const taskFrameContent = fs.readFileSync(taskframePath, 'utf8');
      const taskFrame = JSON.parse(taskFrameContent);

      // Initialize generator
      const basePath = path.dirname(taskframePath);
      const generator = new PerfectPromptGenerator(
        basePath,
        new MockLLMProvider(),
        undefined, // No retrieval provider in this demo
        options.telemetry
      );

      // Generate response
      const result = await generator.generate(taskFrame);

      // Output result
      const output = JSON.stringify(result, null, 2);
      
      if (options.output) {
        fs.writeFileSync(options.output, output);
        console.log(`‚úÖ Result written to ${options.output}`);
      } else {
        console.log(output);
      }

    } catch (error) {
      console.error('‚ùå Generation failed:', error);
      process.exit(1);
    }
  });

program
  .command('ask')
  .description('Quick Q&A generation')
  .argument('<question>', 'Question to ask')
  .option('-d, --domain <domain>', 'Domain (policies, technical_docs, etc.)', 'general')
  .option('-s, --style <style>', 'Style (executive_brief, technical, empathetic)', 'executive_brief')
  .option('-f, --format <format>', 'Output format (json, markdown, text)', 'json')
  .option('--no-cite', 'Disable citation requirements')
  .option('-w, --words <number>', 'Maximum words', '120')
  .option('-o, --output <path>', 'Output file path (default: stdout)')
  .action(async (question, options) => {
    try {
      const generator = new PerfectPromptGenerator(
        process.cwd(),
        new MockLLMProvider()
      );

      const result = await generator.ask(question, options.domain as any, {
        style: options.style as any,
        format: options.format as any,
        must_cite: options.cite,
        max_words: parseInt(options.words)
      });

      const output = JSON.stringify(result.output, null, 2);
      
      if (options.output) {
        fs.writeFileSync(options.output, output);
        console.log(`‚úÖ Answer written to ${options.output}`);
      } else {
        console.log(output);
      }

    } catch (error) {
      console.error('‚ùå Generation failed:', error);
      process.exit(1);
    }
  });

program
  .command('create-frame')
  .description('Create a new TaskFrame from description')
  .argument('<description>', 'Natural language description of the task')
  .option('-o, --output <path>', 'Output TaskFrame file path')
  .action((description, options) => {
    try {
      const partial = Utils.parseNaturalLanguage(description);
      
      const taskFrame = TaskFrameBuilder
        .create(partial.goal || 'qa_with_citations', partial.domain || 'general')
        .setInputs({ question: description })
        .setConstraints(partial.constraints || {
          style: 'executive_brief',
          format: 'json',
          tone: 'neutral',
          length: { max_words: 120 }
        })
        .setContextPolicy({
          retrieval: { k: 6, rerank: true },
          must_cite: true,
          fallback_on_low_confidence: 'insufficient_evidence'
        })
        .setOutputSchema('schemas/qa.json')
        .build();

      const output = JSON.stringify(taskFrame, null, 2);
      
      if (options.output) {
        fs.writeFileSync(options.output, output);
        console.log(`‚úÖ TaskFrame created: ${options.output}`);
      } else {
        console.log(output);
      }

    } catch (error) {
      console.error('‚ùå TaskFrame creation failed:', error);
      process.exit(1);
    }
  });

program
  .command('eval')
  .description('Run evaluation on golden set')
  .argument('<golden-set>', 'Path to golden set JSONL file')
  .option('-v, --version <version>', 'Version tag for this evaluation', 'current')
  .option('-r, --report <path>', 'Generate detailed report file')
  .action(async (goldenSetPath, options) => {
    try {
      const generator = new PerfectPromptGenerator(
        path.dirname(goldenSetPath),
        new MockLLMProvider()
      );

      const summary = await generator.evaluate(goldenSetPath, options.version);

      console.log(`üìä Evaluation Results:`);
      console.log(`   Pass Rate: ${(summary.pass_rate * 100).toFixed(1)}%`);
      console.log(`   Average Score: ${summary.average_score.toFixed(3)}`);
      console.log(`   Total Cost: $${summary.total_cost.toFixed(4)}`);

      if (options.report) {
        const report = generator.generateReport({
          start: new Date(Date.now() - 24 * 60 * 60 * 1000).toISOString(),
          end: new Date().toISOString()
        });
        fs.writeFileSync(options.report, report);
        console.log(`üìÑ Report written to ${options.report}`);
      }

    } catch (error) {
      console.error('‚ùå Evaluation failed:', error);
      process.exit(1);
    }
  });

program
  .command('smoke')
  .description('Run quick smoke test')
  .argument('<golden-set>', 'Path to golden set JSONL file')
  .option('-n, --examples <number>', 'Number of examples to test', '5')
  .action(async (goldenSetPath, options) => {
    try {
      const generator = new PerfectPromptGenerator(
        path.dirname(goldenSetPath),
        new MockLLMProvider()
      );

      const passed = await generator.smokeTest(goldenSetPath, parseInt(options.examples));

      if (passed) {
        console.log('‚úÖ Smoke test PASSED');
        process.exit(0);
      } else {
        console.log('‚ùå Smoke test FAILED');
        process.exit(1);
      }

    } catch (error) {
      console.error('‚ùå Smoke test error:', error);
      process.exit(1);
    }
  });

program
  .command('dashboard')
  .description('Show telemetry dashboard')
  .option('-h, --hours <hours>', 'Time range in hours', '24')
  .action((options) => {
    try {
      const generator = new PerfectPromptGenerator(
        process.cwd(),
        new MockLLMProvider()
      );

      const endTime = new Date();
      const startTime = new Date(endTime.getTime() - parseInt(options.hours) * 60 * 60 * 1000);

      const dashboard = generator.getDashboard({
        start: startTime.toISOString(),
        end: endTime.toISOString()
      });

      console.log('üìä Telemetry Dashboard\n');
      console.log(`Summary (last ${options.hours}h):`);
      console.log(`  Executions: ${dashboard.summary.total_executions}`);
      console.log(`  Success Rate: ${(dashboard.summary.success_rate * 100).toFixed(1)}%`);
      console.log(`  Avg Cost: $${dashboard.summary.avg_cost.toFixed(4)}`);
      console.log(`  Avg Latency: ${dashboard.summary.avg_latency.toFixed(0)}ms`);

    } catch (error) {
      console.error('‚ùå Dashboard error:', error);
      process.exit(1);
    }
  });

program
  .command('init')
  .description('Initialize a new YAFA project')
  .argument('<project-name>', 'Name of the project')
  .action((projectName) => {
    try {
      const projectDir = path.join(process.cwd(), projectName);
      
      // Create directory structure
      const dirs = [
        'styles',
        'exemplars', 
        'schemas',
        'templates',
        'frames',
        'evals'
      ];
      
      for (const dir of dirs) {
        fs.mkdirSync(path.join(projectDir, dir), { recursive: true });
      }

      // Copy example files
      const exampleFiles = [
        'styles/executive_brief.yml',
        'exemplars/policies.qa.jsonl',
        'schemas/qa.json',
        'templates/system.md',
        'templates/user_qa.md',
        'templates/critic.md',
        'evals/golden.qa.jsonl'
      ];

      for (const file of exampleFiles) {
        const sourcePath = path.join(__dirname, file);
        const destPath = path.join(projectDir, file);
        
        if (fs.existsSync(sourcePath)) {
          fs.copyFileSync(sourcePath, destPath);
        }
      }

      console.log(`‚úÖ YAFA project initialized: ${projectDir}`);
      console.log('\nNext steps:');
      console.log(`  cd ${projectName}`);
      console.log('  promptgen ask "What is the travel policy?" --domain policies');

    } catch (error) {
      console.error('‚ùå Initialization failed:', error);
      process.exit(1);
    }
  });

// Error handling
program.configureOutput({
  writeErr: (str) => process.stderr.write(`‚ùå ${str}`)
});

program.parse();
